---
layout: essay
type: essay
title: "데이터를 해석하는 기본 도구들과 기계 학습의 이해"
date: 2025-02-25
published: true
labels:
  - 통계 분석
  - 데이터 해석
  - 기계 학습
---

<img width="100px" class="rounded float-start pe-4" src="../img/250225/datascience.webp">

데이터를 분석하고 해석하는 과정에서 다양한 통계적 개념들이 활용된다. 그중에서도 가설 검정, 신뢰 구간, 그리고 회귀 분석에서의 분산 분해와 결정 계수(R²)는 필수적인 도구이다.  

## 1. 가설 검정 (Hypothesis Testing)

가설 검정은 특정 주장이 통계적으로 의미 있는지를 검토하는 과정이다. 보통 귀무가설(H₀)과 대립가설(H₁)을 설정하여 데이터가 대립가설을 지지하는지를 판단한다.  

- **귀무가설(H₀):** 효과나 차이가 없다는 기본 가정이다.  
- **대립가설(H₁):** 실제로 차이나 효과가 존재한다는 주장이다.  

유의 수준(α)을 설정하고 검정 통계량을 계산한 후, p-값과 비교하여 귀무가설을 기각할지 여부를 결정한다. 일반적으로 α = 0.05를 사용하며, p-값이 이보다 작으면 귀무가설을 기각하고 대립가설을 채택한다. 이를 통해 데이터가 특정 주장을 뒷받침하는지를 검증할 수 있다.  

## 2. 신뢰 구간 (Confidence Interval)

신뢰 구간은 모집단의 모수를 추정할 때 불확실성을 고려하여 설정하는 범위이다. 예를 들어, 95% 신뢰 구간은 "모집단 평균이 이 구간 내에 있을 확률이 95%다"라는 의미로 해석된다.  

신뢰 구간의 구성 요소는 다음과 같다.  
- **추정치:** 보통 표본 평균을 사용한다.  
- **오차 한계(Margin of Error):** 표본의 표준편차와 표본 크기에 따라 결정된다.  

이러한 신뢰 구간은 모수가 특정 구간 내에 존재할 가능성을 제공하며, 단일 값으로 특정할 수 없는 모집단 특성에 대한 추론을 돕는다.  

## 3. 회귀 분석에서의 분산 분해와 결정 계수 (R²)

회귀 분석은 종속 변수와 독립 변수 간의 관계를 모델링하는 기법이다. 이를 통해 데이터의 변동성을 설명하는 데 중요한 요소인 **SST, SSR, SSE** 개념이 등장한다.  

- **SST (Total Sum of Squares):** 전체 데이터의 변동성이다.  
- **SSR (Regression Sum of Squares):** 회귀 모델이 설명하는 변동성이다.  
- **SSE (Error Sum of Squares):** 회귀 모델이 설명하지 못한 잔차(오차)로 인한 변동성이다.  

이들은 다음과 같은 관계를 가진다.  
\[
SST = SSR + SSE
\]  
즉, 데이터의 총 변동성(SST)은 모델이 설명할 수 있는 변동성(SSR)과 설명할 수 없는 변동성(SSE)으로 나뉜다.  

**결정 계수(R²)**는 회귀 모델이 데이터를 얼마나 잘 설명하는지를 나타내는 지표이다.  
\[
R^2 = \frac{SSR}{SST} = 1 - \frac{SSE}{SST}
\]  
\( R^2 \) 값이 1에 가까울수록 모델이 데이터를 잘 설명하며, 예를 들어 \( R^2 = 0.9 \)라면 전체 변동성의 90%가 모델에 의해 설명된다는 의미이다.  

---

## 4. 기계 학습 (Machine Learning)

오늘 학습한 내용 중 하나는 **기계 학습(ML)**의 기본 개념과 선형 회귀(Linear Regression) 모델이다.  

### 4.1 선형 회귀 (Linear Regression with PyTorch)

- **목적:**  
  독립 변수(피처)와 종속 변수(타겟) 간의 관계를 선형(1차 함수)으로 모델링하여 예측하는 지도 학습 알고리즘이다.  

- **핵심 개념:**  
  - **선형(Linear):** 직선 형태의 관계  
  - **비선형(Non-linear):** 곡선 형태로 더 복잡한 관계를 모델링할 때 사용됨  
  - **회귀(Regression):** 독립 변수가 종속 변수에 미치는 영향을 분석하여 평균적인 경향을 예측  

- **변수의 역할:**  
  - **독립 변수(Feature):** 원인 또는 입력값 (예: 키)  
  - **종속 변수(Target):** 결과 또는 예측값 (예: 몸무게)  

- **PyTorch 활용:**  
  파이썬 기반의 딥러닝 라이브러리인 PyTorch를 사용하여 선형 회귀 모델을 구축하고 학습할 수 있다.  

### 4.2 기계 학습 모델의 학습 과정

- **정의:**  
  데이터와 수학적 모델을 활용하여 **최적의 관계**를 찾고, 오차(실제 값과 예측 값의 차이)를 최소화하는 것이 목표이다.  

- **시각적 예시:**  
  - 데이터 포인트는 좌표 공간에서 개별 특성을 나타냄  
  - 최적의 선(y = ax + b)을 찾는 것이 핵심  

- **주의점:**  
  - **과적합(Overfitting):** 학습 데이터에 과하게 맞춰져 새로운 데이터에 대한 성능이 저하될 위험이 있음  

---

## 5. 오늘의 회고

### 5.1 배운 점
- 기계 학습에서 **선형 회귀 모델**이 어떻게 작동하는지, 그리고 오차 최소화의 개념이 명확해졌다.  

### 5.2 어려운 점/개선할 점
- 과적합 문제와 모델 평가 지표에 대해 추가적인 학습이 필요하다.  

### 5.3 향후 실천 계획
- PyTorch를 활용하여 선형 회귀 모델을 직접 구현하고 다양한 평가 기법을 실험해볼 예정이다.  

### 5.4 함께 나누고 싶은 점
- 실습을 통해 **이론과 실전 사이의 연결고리를 찾는 과정이 중요하다**는 점을 강조하고 싶다.  

---

## 6. 내일의 학습 계획

- **PyTorch를 활용한 선형 회귀 모델 구현 실습**  
- **과적합 방지를 위한 정규화 기법 학습**  

---

## 7. 참고자료

- PyTorch 공식 문서  
- 머신러닝 강의 자료  
- 관련 블로그 및 논문  

